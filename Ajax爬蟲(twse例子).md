# å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™çˆ¬èŸ²é–‹ç™¼ç­†è¨˜ ğŸ“š

> å®Œæ•´è¨˜éŒ„å¦‚ä½•å¾é›¶é–‹å§‹é–‹ç™¼ MOPS çˆ¬èŸ²çš„æ‰€æœ‰çŸ¥è­˜

---

## ğŸ“– ç›®éŒ„

1. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
2. [é–‹ç™¼æµç¨‹](#é–‹ç™¼æµç¨‹)
3. [å·²å®Œæˆçš„çˆ¬èŸ²](#å·²å®Œæˆçš„çˆ¬èŸ²)
4. [æŠ€è¡“é¸æ“‡ï¼šAJAX vs Selenium](#æŠ€è¡“é¸æ“‡ajax-vs-selenium)
5. [é˜²å°é–æŠ€å·§](#é˜²å°é–æŠ€å·§)
6. [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)
7. [æ“´å±•æ–¹å‘](#æ“´å±•æ–¹å‘)

---

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€éº¼æ˜¯ AJAX çˆ¬èŸ²ï¼Ÿ

ç•¶ä½ åœ¨ç¶²é ä¸Šè¼¸å…¥å…¬å¸ä»£è™ŸæŸ¥è©¢æ™‚ï¼Œç€è¦½å™¨å¯¦éš›ä¸Šæ˜¯ï¼š

```
1. JavaScript ç™¼é€ AJAX è«‹æ±‚åˆ°ä¼ºæœå™¨
   â†“
2. ä¼ºæœå™¨å›å‚³ HTML æˆ– JSON è³‡æ–™
   â†“
3. JavaScript å°‡è³‡æ–™é¡¯ç¤ºåœ¨ç¶²é ä¸Š
```

**AJAX çˆ¬èŸ²çš„åŸç†ï¼šç›´æ¥è·³éæ­¥é©Ÿ 1ï¼Œç”¨ç¨‹å¼ç™¼é€ç›¸åŒçš„è«‹æ±‚**

### ç‚ºä»€éº¼ä¸ç”¨ Seleniumï¼Ÿ

| ç‰¹æ€§ | **AJAX çˆ¬èŸ²** | **Selenium** |
|------|--------------|--------------|
| é€Ÿåº¦ | âš¡ 0.5 ç§’ | ğŸŒ 10 ç§’ |
| è¨˜æ†¶é«” | ğŸ’š 20 MB | ğŸ”´ 300 MB |
| ç¨‹å¼ç¢¼ | âœ… 30 è¡Œ | âŒ 80 è¡Œ |
| é©ç”¨å ´æ™¯ | 90% çš„ç¶²ç«™ | éœ€è¦ JS åŸ·è¡Œã€ç™»å…¥ |

**çµè«–ï¼šå…¬é–‹è³‡è¨Šè§€æ¸¬ç«™å®Œå…¨ä¸éœ€è¦ Selenium**

---

## é–‹ç™¼æµç¨‹

### æ­¥é©Ÿ 1ï¸âƒ£ï¼šæ‰¾åˆ° AJAX è«‹æ±‚

1. æ‰“é–‹ Chrome DevTools (F12)
2. åˆ‡æ›åˆ° **Network** é¢æ¿
3. åœ¨ç¶²é ä¸Šæ“ä½œï¼ˆä¾‹å¦‚è¼¸å…¥å…¬å¸ä»£è™ŸæŸ¥è©¢ï¼‰
4. æ‰¾åˆ° XHR/Fetch é¡å‹çš„è«‹æ±‚

**é—œéµè³‡è¨Šï¼š**
- Request URLï¼š`https://mopsov.twse.com.tw/mops/web/ajax_xxxxx`
- Request Methodï¼š`POST` æˆ– `GET`
- Payloadï¼šè«‹æ±‚åƒæ•¸

### æ­¥é©Ÿ 2ï¸âƒ£ï¼šæˆªåœ–çµ¦ AI

éœ€è¦æä¾›çš„è³‡è¨Šï¼š
1. **Headers é é¢**ï¼šRequest URLã€Request Method
2. **Payload é é¢**ï¼šæ‰€æœ‰åƒæ•¸å’Œå€¼
3. **Response é é¢**ï¼ˆé¸ç”¨ï¼‰ï¼šå›å‚³çš„è³‡æ–™æ ¼å¼

### æ­¥é©Ÿ 3ï¸âƒ£ï¼šAI ç”¢ç”Ÿç¨‹å¼ç¢¼

AI æœƒè‡ªå‹•ç”¢ç”Ÿï¼š
```python
import requests
from bs4 import BeautifulSoup

class Crawler:
    def __init__(self):
        self.base_url = "ä½ æä¾›çš„ URL"
        self.headers = {...}
    
    def get_data(self, stock_code):
        payload = {
            # ä½ æä¾›çš„åƒæ•¸
        }
        response = requests.post(self.base_url, data=payload, headers=self.headers)
        return self.parse_response(response.text)
    
    def parse_response(self, html):
        # è‡ªå‹•è§£æ HTML
        soup = BeautifulSoup(html, 'html.parser')
        # ...
```

### æ­¥é©Ÿ 4ï¸âƒ£ï¼šæ¸¬è©¦èˆ‡èª¿æ•´

```bash
python your_crawler.py
```

å¦‚æœæœ‰å•é¡Œï¼ˆä¾‹å¦‚ PDF è§£æéŒ¯èª¤ï¼‰ï¼Œæä¾›ï¼š
- éŒ¯èª¤è¨Šæ¯
- Response çš„ HTML ç¯„ä¾‹
- é æœŸçµæœ

---

## å·²å®Œæˆçš„çˆ¬èŸ²

### 1. æ³•äººèªªæ˜æœƒçˆ¬èŸ² ğŸ“Š

**æª”æ¡ˆï¼š** `mops_crawler.py`

**åŠŸèƒ½ï¼š**
- æŸ¥è©¢æ³•èªªæœƒæ™‚ç¨‹ã€åœ°é»
- å–å¾—ç°¡å ±æª”æ¡ˆï¼ˆä¸­è‹±æ–‡ PDFï¼‰
- å½±éŸ³é€£çµã€å…¬å¸ç¶²ç«™

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from mops_crawler import MOPSCrawler

crawler = MOPSCrawler()
result = crawler.get_investor_conference('2330')

print(f"æ³•èªªæœƒæ—¥æœŸ: {result['conferences'][0]['date']}")
print(f"ç°¡å ±é€£çµ: {result['conferences'][0]['files']['chinese_pdf']}")
```

**API è³‡è¨Šï¼š**
- URL: `https://mopsov.twse.com.tw/mops/web/ajax_t100sb07_1`
- Method: `POST`
- ä¸»è¦åƒæ•¸: `co_id`ï¼ˆå…¬å¸ä»£è™Ÿï¼‰

---

### 2. è²¡å ±çˆ¬èŸ² ğŸ’°

**æª”æ¡ˆï¼š** `financial_crawler.py`

**åŠŸèƒ½ï¼š**
- åˆä½µç¶œåˆæç›Šè¡¨
- ç‡Ÿæ”¶ã€æˆæœ¬ã€æ¯›åˆ©ã€æ·¨åˆ©
- æ¯è‚¡ç›ˆé¤˜ (EPS)
- å­£åº¦èˆ‡ç´¯è¨ˆè³‡æ–™

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from financial_crawler import FinancialStatementCrawler

crawler = FinancialStatementCrawler()

# æŸ¥è©¢æœ€æ–°å­£åº¦
result = crawler.get_income_statement('2330')

# æŸ¥è©¢ç‰¹å®šå­£åº¦
result = crawler.get_income_statement('2330', year='114', season='1')

print(f"ç‡Ÿæ¥­æ”¶å…¥: {result['summary']['revenue'][0]['amount']:,.0f} åƒå…ƒ")
print(f"æœ¬æœŸæ·¨åˆ©: {result['summary']['net_income'][0]['amount']:,.0f} åƒå…ƒ")
print(f"EPS: {result['summary']['basic_eps'][0]['amount']:.2f}")
```

**API è³‡è¨Šï¼š**
- URL: `https://mopsov.twse.com.tw/mops/web/ajax_t164sb04`
- Method: `POST`
- ä¸»è¦åƒæ•¸: `co_id`, `year`, `season`

---

### 3. æœˆç‡Ÿæ”¶çˆ¬èŸ² ğŸ“ˆ

**æª”æ¡ˆï¼š** `monthly_revenue_crawler.py`

**åŠŸèƒ½ï¼š**
- æœ¬æœˆç‡Ÿæ”¶
- å»å¹´åŒæœŸæ¯”è¼ƒ
- å¹´å¢ç‡
- æœ¬å¹´ç´¯è¨ˆ

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from monthly_revenue_crawler import MonthlyRevenueCrawler

crawler = MonthlyRevenueCrawler()

# æŸ¥è©¢æœ€æ–°æœˆç‡Ÿæ”¶
result = crawler.get_monthly_revenue('2330')

# æŸ¥è©¢ç‰¹å®šæœˆä»½
result = crawler.get_monthly_revenue('2330', year='114', month='9')

print(f"æœ¬æœˆç‡Ÿæ”¶: {result['current_month']:,.0f} åƒå…ƒ")
print(f"æœˆå¢æ¸›: {result['mom_change_pct']}%")
print(f"å¹´ç´¯è¨ˆ: {result['ytd_current']:,.0f} åƒå…ƒ")
```

**API è³‡è¨Šï¼š**
- URL: `https://mopsov.twse.com.tw/mops/web/ajax_t05st10_ifrs`
- Method: `POST`
- ä¸»è¦åƒæ•¸: `co_id`, `year`, `month`

---

### 4. é‡å¤§è¨Šæ¯çˆ¬èŸ² ğŸ“°

**æª”æ¡ˆï¼š** `announcement_crawler.py`

**åŠŸèƒ½ï¼š**
- å…¬å¸é‡å¤§è¨Šæ¯å…¬å‘Š
- ç™¼è¨€æ—¥æœŸã€æ™‚é–“
- å…¬å‘Šä¸»æ—¨

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from announcement_crawler import AnnouncementCrawler

crawler = AnnouncementCrawler()

# æŸ¥è©¢ä»Šå¹´åº¦æ‰€æœ‰å…¬å‘Š
result = crawler.get_announcements('2330', year='114')

# æŸ¥è©¢ç‰¹å®šæœˆä»½
result = crawler.get_announcements('2330', year='114', month='10')

print(f"å…± {result['total_count']} å‰‡è¨Šæ¯")
for ann in result['announcements'][:5]:
    print(f"[{ann['date']}] {ann['subject']}")
```

**API è³‡è¨Šï¼š**
- URL: `https://mopsov.twse.com.tw/mops/web/ajax_t05st01`
- Method: `POST`
- ä¸»è¦åƒæ•¸: `co_id`, `year`, `month`, `b_date`, `e_date`

---

### 5. æ•´åˆåˆ†æå·¥å…· ğŸ¯

**æª”æ¡ˆï¼š** `integrated_example.py`

**åŠŸèƒ½ï¼š**
- åŒæ™‚æŸ¥è©¢æ³•èªªæœƒ + è²¡å ±
- è‡ªå‹•ç”¢ç”ŸæŠ•è³‡åˆ†æå ±å‘Š
- å¤šå…¬å¸æ¯”è¼ƒ

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from integrated_example import CompanyAnalyzer

analyzer = CompanyAnalyzer()

# ç”¢ç”Ÿå®Œæ•´å ±å‘Š
analyzer.create_investment_report('2330')

# æ¯”è¼ƒå¤šå®¶å…¬å¸
analyzer.compare_companies(['2330', '2317', '2454'])
```

---

## æŠ€è¡“é¸æ“‡ï¼šAJAX vs Selenium

### ğŸ“Š æ•ˆèƒ½å°æ¯”

```
ä»»å‹™ï¼šæŸ¥è©¢ä¸€å®¶å…¬å¸çš„é‡å¤§è¨Šæ¯

ç›´æ¥ AJAX:
â”œâ”€ å•Ÿå‹•æ™‚é–“: 0 ç§’
â”œâ”€ è«‹æ±‚æ™‚é–“: 0.3 ç§’
â”œâ”€ è§£ææ™‚é–“: 0.2 ç§’
â””â”€ ç¸½è¨ˆ: 0.5 ç§’

Selenium:
â”œâ”€ å•Ÿå‹•ç€è¦½å™¨: 2-3 ç§’
â”œâ”€ è¼‰å…¥ç¶²é : 3-5 ç§’
â”œâ”€ è¼¸å…¥ä¸¦æŸ¥è©¢: 1-2 ç§’
â”œâ”€ ç­‰å¾…çµæœ: 2-3 ç§’
â””â”€ ç¸½è¨ˆ: 8-13 ç§’

é€Ÿåº¦å·®ç•°: 16-26 å€ï¼
```

### âœ… ä»€éº¼æ™‚å€™ç”¨ AJAXï¼Ÿ

**é©ç”¨å ´æ™¯ï¼š**
- âœ… è³‡æ–™åœ¨ AJAX response ä¸­
- âœ… ä¸éœ€è¦ç™»å…¥
- âœ… æ²’æœ‰ CAPTCHA
- âœ… ä¸éœ€è¦åŸ·è¡Œ JavaScript

**å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™å®Œå…¨ç¬¦åˆ â†’ ç”¨ AJAX å°±å°äº†**

### âš ï¸ ä»€éº¼æ™‚å€™å¿…é ˆç”¨ Seleniumï¼Ÿ

**å¿…é ˆç”¨ Selenium çš„æƒ…æ³ï¼š**

1. **JavaScript å‹•æ…‹è¼‰å…¥**
   ```
   è³‡æ–™å®Œå…¨ç”± JS ç”¢ç”Ÿï¼Œæ‰¾ä¸åˆ° API
   â†’ ä¾‹å¦‚ï¼šè‚¡åƒ¹å³æ™‚åœ–è¡¨ï¼ˆCanvas ç¹ªè£½ï¼‰
   ```

2. **éœ€è¦ç™»å…¥**
   ```
   éœ€è¦å¸³è™Ÿå¯†ç¢¼ã€è™•ç† Session
   â†’ ä¾‹å¦‚ï¼šç¶²è·¯éŠ€è¡Œã€åˆ¸å•†ä¸‹å–®ç³»çµ±
   ```

3. **æœ‰ CAPTCHA é©—è­‰ç¢¼**
   ```
   éœ€è¦äººå·¥é»é¸åœ–ç‰‡
   â†’ ä¾‹å¦‚ï¼šPTT ç™»å…¥ã€æŸäº›å”®ç¥¨ç¶²ç«™
   ```

4. **è¤‡é›œçš„åçˆ¬èŸ²æ©Ÿåˆ¶**
   ```
   æª¢æŸ¥ç€è¦½å™¨æŒ‡ç´‹ã€Canvasã€WebGL
   â†’ ä¾‹å¦‚ï¼šæŸäº›é›»å•†ç¶²ç«™
   ```

### ğŸ’¡ å¦‚ä½•åˆ¤æ–·è¦ç”¨å“ªç¨®ï¼Ÿ

**ç°¡å–®æ¸¬è©¦æ³•ï¼š**

```python
# 1. å…ˆè©¦è©¦ç›´æ¥ AJAX
import requests
response = requests.get('https://ç›®æ¨™ç¶²ç«™.com/api')

# 2. æª¢æŸ¥ response
if response.status_code == 200 and len(response.text) > 0:
    print("âœ… ç”¨ AJAX å°±å¯ä»¥")
else:
    print("âŒ å¯èƒ½éœ€è¦ Selenium")
```

---

## é˜²å°é–æŠ€å·§

### ğŸ›¡ï¸ åŸºæœ¬é˜²è­·

#### 1. åŠ ä¸Šå®Œæ•´çš„ Headers

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Referer': 'https://mopsov.twse.com.tw/mops/web/t05st01',
    'Origin': 'https://mopsov.twse.com.tw',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',
    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8',
}
```

**ç‚ºä»€éº¼è¦åŠ ï¼Ÿ**
- User-Agentï¼šå‘Šè¨´ä¼ºæœå™¨ä½ æ˜¯ç€è¦½å™¨
- Refererï¼šå‘Šè¨´ä¼ºæœå™¨ä½ å¾å“ªå€‹é é¢ä¾†
- Originï¼šå‘Šè¨´ä¼ºæœå™¨è«‹æ±‚ä¾†æº

#### 2. æ§åˆ¶è«‹æ±‚é »ç‡

```python
import time
import random

for code in stock_codes:
    result = crawler.get_data(code)
    
    # éš¨æ©Ÿå»¶é² 1-3 ç§’
    time.sleep(random.uniform(1, 3))
    
    # æ¯ 100 å€‹è«‹æ±‚ä¼‘æ¯ä¸€ä¸‹
    if i % 100 == 0:
        print("ä¼‘æ¯ 1 åˆ†é˜...")
        time.sleep(60)
```

**å»ºè­°é »ç‡ï¼š**
- âœ… æ¯ç§’ 1-2 å€‹è«‹æ±‚ï¼šå®Œå…¨æ²’å•é¡Œ
- âš ï¸ æ¯ç§’ 10+ å€‹è«‹æ±‚ï¼šå¯èƒ½è¢«é™é€Ÿ
- âŒ æ¯ç§’ 100+ å€‹è«‹æ±‚ï¼šæœƒè¢«å° IP

#### 3. ä½¿ç”¨ Session

```python
import requests

# âœ… ä½¿ç”¨ Session (è‡ªå‹•è™•ç† Cookie)
session = requests.Session()
response = session.post(url, data=payload)

# âŒ ä¸è¦æ¯æ¬¡éƒ½å»ºç«‹æ–°é€£ç·š
response = requests.post(url, data=payload)
```

#### 4. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦

```python
import time

def safe_request(url, data, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.post(url, data=data, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"è«‹æ±‚å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)  # ç­‰ 5 ç§’å†è©¦
            else:
                return None
```

### ğŸš¨ AJAX è«‹æ±‚æœƒè¢«æ“‹å—ï¼Ÿ

#### å¯èƒ½è¢«æ“‹çš„åŸå› ï¼š

| æª¢æŸ¥é …ç›® | MOPS æ˜¯å¦æª¢æŸ¥ | è§£æ±ºæ–¹æ³• |
|---------|--------------|---------|
| User-Agent | âŒ ä¸æª¢æŸ¥ | åŠ å‡çš„ä¹Ÿè¡Œ |
| Referer | âŒ ä¸æª¢æŸ¥ | åŠ å‡çš„ä¹Ÿè¡Œ |
| Cookie/Session | âŒ ä¸éœ€è¦ | - |
| CSRF Token | âŒ æ²’æœ‰ | - |
| IP é™åˆ¶ | âš ï¸ å¤ªé »ç¹æœƒé™é€Ÿ | åŠ å»¶é² |
| Rate Limiting | âš ï¸ æœ‰ï¼Œä½†å¾ˆå¯¬é¬† | æ¯ç§’ 1-2 æ¬¡å°±å¥½ |
| JavaScript æª¢æŸ¥ | âŒ æ²’æœ‰ | - |

**çµè«–ï¼šMOPS å¹¾ä¹ä¸æœƒæ“‹ä½ ï¼Œä½†è«‹å‹å–„ä½¿ç”¨**

### ğŸ“ å¯¦æˆ°å»ºè­°

#### âœ… å¥½çš„åšæ³•

```python
# 1. åˆç†çš„å»¶é²
time.sleep(1)

# 2. éŒ¯èª¤è™•ç†
try:
    result = crawler.get_data(code)
except Exception as e:
    print(f"å¤±æ•—: {e}")

# 3. å„²å­˜ä¸­é–“çµæœ
if i % 50 == 0:
    save_to_file(results)

# 4. é¡¯ç¤ºé€²åº¦
print(f"é€²åº¦: {i}/{total}")
```

#### âŒ ä¸å¥½çš„åšæ³•

```python
# 1. æ²’æœ‰å»¶é²
for code in range(1000, 9999):
    crawler.get_data(code)  # ç˜‹ç‹‚è«‹æ±‚

# 2. æ²’æœ‰éŒ¯èª¤è™•ç†
result = crawler.get_data(code)  # ä¸€å‡ºéŒ¯å°±çˆ†æ‰

# 3. ä¸å„²å­˜ä¸­é–“çµæœ
# è·‘äº† 3 å°æ™‚å¾Œç¨‹å¼ç•¶æ‰ï¼Œä¸€åˆ‡é‡ä¾†

# 4. æ²’æœ‰ timeout
response = requests.post(url)  # å¯èƒ½æ°¸é ç­‰ä¸‹å»
```

---

## å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•çŸ¥é“ä¸€å€‹ç¶²ç«™ç”¨ AJAX é‚„æ˜¯å‚³çµ±æ–¹å¼ï¼Ÿ

**æ–¹æ³•ï¼šé–‹ DevTools çœ‹ Network**

```
1. F12 é–‹å•Ÿ DevTools
2. åˆ‡æ›åˆ° Network é¢æ¿
3. æ“ä½œç¶²é ï¼ˆä¾‹å¦‚é»æ“ŠæŒ‰éˆ•ï¼‰
4. è§€å¯Ÿï¼š
   - æœ‰ XHR/Fetch è«‹æ±‚ â†’ AJAX
   - æ•´å€‹é é¢é‡æ–°è¼‰å…¥ â†’ å‚³çµ±æ–¹å¼
```

### Q2: ç‚ºä»€éº¼æˆ‘çš„ç¨‹å¼æŠ“ä¸åˆ°è³‡æ–™ï¼Ÿ

**æª¢æŸ¥æ¸…å–®ï¼š**

```python
# 1. æª¢æŸ¥ status code
print(response.status_code)  # æ‡‰è©²æ˜¯ 200

# 2. æª¢æŸ¥ response å…§å®¹
print(response.text[:500])  # çœ‹å‰ 500 å­—å…ƒ

# 3. æª¢æŸ¥åƒæ•¸æ˜¯å¦æ­£ç¢º
print(payload)  # ç¢ºèªåƒæ•¸åç¨±å’Œå€¼

# 4. æª¢æŸ¥æ˜¯å¦éœ€è¦ headers
headers = {
    'User-Agent': 'Mozilla/5.0...',
    'Referer': 'https://...'
}
```

### Q3: å¦‚ä½•è™•ç†ä¸­æ–‡ç·¨ç¢¼å•é¡Œï¼Ÿ

```python
# âœ… æ­£ç¢ºåšæ³•
response = requests.post(url, data=payload)
response.encoding = 'utf-8'  # æŒ‡å®šç·¨ç¢¼
html = response.text

# å„²å­˜ JSON æ™‚
with open('data.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
```

### Q4: æ‰¹æ¬¡æŸ¥è©¢æ™‚å¦‚ä½•é¿å…è¢«å°ï¼Ÿ

```python
import time
import random

def batch_query_safe(stock_codes):
    results = {}
    total = len(stock_codes)
    
    for i, code in enumerate(stock_codes, 1):
        print(f"é€²åº¦: {i}/{total} - æ­£åœ¨æŸ¥è©¢ {code}")
        
        # æŸ¥è©¢
        result = crawler.get_data(code)
        results[code] = result
        
        # éš¨æ©Ÿå»¶é² 1-3 ç§’
        delay = random.uniform(1, 3)
        time.sleep(delay)
        
        # æ¯ 50 å€‹å„²å­˜ä¸€æ¬¡
        if i % 50 == 0:
            save_results(results, f'backup_{i}.json')
        
        # æ¯ 100 å€‹ä¼‘æ¯ 1 åˆ†é˜
        if i % 100 == 0:
            print("ä¼‘æ¯ 1 åˆ†é˜...")
            time.sleep(60)
    
    return results
```

### Q5: å¦‚ä½•è§£æè¤‡é›œçš„ HTML çµæ§‹ï¼Ÿ

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser')

# æ–¹æ³• 1: æ‰¾ç‰¹å®š class
table = soup.find('table', class_='hasBorder')

# æ–¹æ³• 2: æ‰¾ç‰¹å®šæ–‡å­—
cell = soup.find('td', string='ç‡Ÿæ¥­æ”¶å…¥')

# æ–¹æ³• 3: ç”¨ CSS selector
rows = soup.select('table.hasBorder tr')

# æ–¹æ³• 4: æ‰¾æ‰€æœ‰ç¬¦åˆçš„
all_tables = soup.find_all('table')

# æ–¹æ³• 5: å–æ–‡å­—å…§å®¹
text = cell.get_text(strip=True)

# æ–¹æ³• 6: å–å±¬æ€§
href = link.get('href')
```

### Q6: è³‡æ–™è§£æéŒ¯èª¤æ€éº¼è¾¦ï¼Ÿ

**é™¤éŒ¯æ­¥é©Ÿï¼š**

```python
# 1. å…ˆçœ‹åŸå§‹ HTML
print(response.text)

# 2. æ‰¾åˆ°ä½ è¦çš„éƒ¨åˆ†
soup = BeautifulSoup(html, 'html.parser')
table = soup.find('table', class_='hasBorder')
print(table)

# 3. é€æ­¥æ¸¬è©¦è§£æé‚è¼¯
rows = table.find_all('tr')
print(f"æ‰¾åˆ° {len(rows)} è¡Œ")

for i, row in enumerate(rows[:3]):  # åªçœ‹å‰ 3 è¡Œ
    print(f"\nç¬¬ {i} è¡Œ:")
    cells = row.find_all('td')
    for j, cell in enumerate(cells):
        print(f"  Cell {j}: {cell.get_text(strip=True)}")
```

---

## æ“´å±•æ–¹å‘

### ğŸ¯ å¯æ–°å¢çš„çˆ¬èŸ²æ¨¡çµ„

å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™é‚„æœ‰å¾ˆå¤šè³‡æ–™å¯ä»¥çˆ¬ï¼š

| æ¨¡çµ„ | é é¢ | API | é›£åº¦ |
|-----|------|-----|------|
| è‚¡æ±æœƒè³‡è¨Š | t05st02 | ajax_t05st02 | â­ ç°¡å–® |
| è‘£ç›£äº‹æŒè‚¡ | t05st03 | ajax_t05st03 | â­ ç°¡å–® |
| è³‡ç”¢è² å‚µè¡¨ | t164sb01 | ajax_t164sb01 | â­â­ ä¸­ç­‰ |
| ç¾é‡‘æµé‡è¡¨ | t164sb05 | ajax_t164sb05 | â­â­ ä¸­ç­‰ |
| è‚¡åˆ©æ”¿ç­– | t05st09 | ajax_t05st09 | â­ ç°¡å–® |
| ç‡Ÿé‹æ¦‚æ³ | t100sb06 | ajax_t100sb06 | â­â­ ä¸­ç­‰ |

### ğŸ“ é–‹ç™¼æ–°æ¨¡çµ„çš„æ­¥é©Ÿ

**æ­¥é©Ÿ 1ï¼šæ‰¾åˆ°é é¢**
```
https://mopsov.twse.com.tw/mops/web/t05st02
```

**æ­¥é©Ÿ 2ï¼šé–‹å•Ÿ DevToolsï¼Œæ‰¾åˆ° AJAX è«‹æ±‚**
```
Network â†’ XHR/Fetch â†’ ajax_t05st02
```

**æ­¥é©Ÿ 3ï¼šæˆªåœ– Headers å’Œ Payload**
- Request URL
- Request Method
- æ‰€æœ‰åƒæ•¸

**æ­¥é©Ÿ 4ï¼šçµ¦ AI æˆ–åƒè€ƒç¾æœ‰ç¨‹å¼ç¢¼**
```python
# è¤‡è£½ announcement_crawler.py
# ä¿®æ”¹ URL å’Œåƒæ•¸
# èª¿æ•´è§£æé‚è¼¯
```

### ğŸš€ é€²éšæ‡‰ç”¨

#### 1. å»ºç«‹å®Œæ•´çš„è³‡æ–™åº«

```python
import sqlite3

# å»ºç«‹è³‡æ–™åº«
conn = sqlite3.connect('mops_data.db')
cursor = conn.cursor()

# å»ºç«‹è¡¨æ ¼
cursor.execute('''
    CREATE TABLE IF NOT EXISTS financial_statements (
        company_code TEXT,
        period TEXT,
        revenue REAL,
        net_income REAL,
        eps REAL,
        query_date TEXT,
        PRIMARY KEY (company_code, period)
    )
''')

# æ’å…¥è³‡æ–™
def save_to_db(result):
    cursor.execute('''
        INSERT OR REPLACE INTO financial_statements 
        VALUES (?, ?, ?, ?, ?, datetime('now'))
    ''', (
        result['company_code'],
        result['period'],
        result['summary']['revenue'][0]['amount'],
        result['summary']['net_income'][0]['amount'],
        result['summary']['basic_eps'][0]['amount']
    ))
    conn.commit()
```

#### 2. å®šæœŸè‡ªå‹•æ›´æ–°

```python
import schedule
import time

def daily_update():
    """æ¯å¤©è‡ªå‹•æ›´æ–°é—œæ³¨çš„å…¬å¸"""
    watch_list = ['2330', '2317', '2454']
    
    for code in watch_list:
        # æ›´æ–°æœˆç‡Ÿæ”¶
        revenue = revenue_crawler.get_monthly_revenue(code)
        save_to_db(revenue)
        
        # æ›´æ–°é‡å¤§è¨Šæ¯
        announcements = announcement_crawler.get_announcements(code)
        check_important_news(announcements)
        
        time.sleep(1)

# æ¯å¤©æ—©ä¸Š 9 é»åŸ·è¡Œ
schedule.every().day.at("09:00").do(daily_update)

while True:
    schedule.run_pending()
    time.sleep(60)
```

#### 3. Line é€šçŸ¥æ•´åˆ

```python
import requests

def send_line_notify(message):
    """ç™¼é€ Line é€šçŸ¥"""
    token = 'YOUR_LINE_NOTIFY_TOKEN'
    
    headers = {
        'Authorization': f'Bearer {token}'
    }
    
    data = {
        'message': message
    }
    
    requests.post(
        'https://notify-api.line.me/api/notify',
        headers=headers,
        data=data
    )

# ä½¿ç”¨ç¯„ä¾‹
result = crawler.get_announcements('2330')
if result['total_count'] > 0:
    latest = result['announcements'][0]
    message = f"\nğŸ“¢ å°ç©é›»æœ€æ–°å…¬å‘Š\n{latest['subject']}"
    send_line_notify(message)
```

#### 4. è¦–è¦ºåŒ–åˆ†æ

```python
import matplotlib.pyplot as plt
import pandas as pd

def plot_revenue_trend(stock_code, periods):
    """ç¹ªè£½ç‡Ÿæ”¶è¶¨å‹¢åœ–"""
    revenues = []
    labels = []
    
    for period in periods:
        result = financial_crawler.get_income_statement(
            stock_code, 
            year=period['year'], 
            season=period['season']
        )
        revenues.append(result['summary']['revenue'][0]['amount'])
        labels.append(f"{period['year']}Q{period['season']}")
    
    plt.figure(figsize=(10, 6))
    plt.plot(labels, revenues, marker='o', linewidth=2)
    plt.title(f'{stock_code} ç‡Ÿæ”¶è¶¨å‹¢')
    plt.xlabel('æœŸé–“')
    plt.ylabel('ç‡Ÿæ”¶ï¼ˆåƒå…ƒï¼‰')
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f'{stock_code}_revenue_trend.png', dpi=300)
    print(f"åœ–è¡¨å·²å„²å­˜")
```

---

## ğŸ“ å­¸ç¿’è³‡æº

### Python çˆ¬èŸ²åŸºç¤

- **requests æ–‡ä»¶**: https://requests.readthedocs.io/
- **BeautifulSoup æ–‡ä»¶**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **æ­£è¦è¡¨é”å¼æ¸¬è©¦**: https://regex101.com/

### ç›¸é—œå·¥å…·

- **JSON æ ¼å¼åŒ–**: https://jsonformatter.org/
- **Chrome DevTools æ•™å­¸**: https://developer.chrome.com/docs/devtools/

### é€²éšä¸»é¡Œ

- **requests-html**: å¯åŸ·è¡Œ JavaScript çš„è¼•é‡ç´šæ–¹æ¡ˆ
- **Scrapy**: å°ˆæ¥­çˆ¬èŸ²æ¡†æ¶
- **asyncio + aiohttp**: ç•°æ­¥çˆ¬èŸ²ï¼Œé€Ÿåº¦æ›´å¿«

---

## âš–ï¸ æ³•å¾‹èˆ‡é“å¾·

### âœ… åˆæ³•ä½¿ç”¨

- å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™çš„è³‡æ–™æ˜¯å…¬é–‹çš„
- å€‹äººç ”ç©¶ã€å­¸ç¿’ç”¨é€”
- ä¸éåº¦é »ç¹è«‹æ±‚
- éµå®ˆ robots.txt

### âŒ ç¦æ­¢è¡Œç‚º

- å•†æ¥­è²©å”®çˆ¬å–çš„è³‡æ–™
- éåº¦é »ç¹è«‹æ±‚é€ æˆä¼ºæœå™¨è² æ“”
- ç”¨æ–¼é•æ³•ç›®çš„ï¼ˆä¾‹å¦‚å…§ç·šäº¤æ˜“ï¼‰
- çªç ´ç™»å…¥æ©Ÿåˆ¶çˆ¬å–éå…¬é–‹è³‡æ–™

### ğŸ’¡ å»ºè­°

1. **å‹å–„çˆ¬èŸ²**: åŠ ä¸Šé©ç•¶å»¶é²
2. **æ¨™æ˜èº«åˆ†**: User-Agent å¯ä»¥åŠ ä¸Šä½ çš„è¯çµ¡æ–¹å¼
3. **å°Šé‡ç¶²ç«™**: ä¸è¦é€ æˆä¼ºæœå™¨è² æ“”
4. **åˆç†ä½¿ç”¨**: åƒ…ä¾›å€‹äººç ”ç©¶èˆ‡å­¸ç¿’

---

## ğŸ“Œ å¿«é€Ÿåƒè€ƒ

### åŸºæœ¬æ¨¡æ¿

```python
import requests
from bs4 import BeautifulSoup
import time

class MOPSCrawler:
    def __init__(self):
        self.base_url = "YOUR_API_URL"
        self.headers = {
            'User-Agent': 'Mozilla/5.0...',
            'Referer': 'https://mopsov.twse.com.tw/...'
        }
    
    def get_data(self, stock_code):
        payload = {
            'co_id': stock_code,
            'step': '1',
            # ... å…¶ä»–åƒæ•¸
        }
        
        try:
            response = requests.post(
                self.base_url,
                data=payload,
                headers=self.headers,
                timeout=30
            )
            response.raise_for_status()
            return self.parse_response(response.text)
        except Exception as e:
            print(f"éŒ¯èª¤: {e}")
            return None
    
    def parse_response(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        # è§£æé‚è¼¯
        return result
    
    def batch_query(self, stock_codes):
        results = {}
        for code in stock_codes:
            result = self.get_data(code)
            results[code] = result
            time.sleep(1)  # é‡è¦ï¼
        return results
```

### å¸¸ç”¨å…¬å¸ä»£è™Ÿ

```python
# æ¬Šå€¼è‚¡
LARGE_CAPS = {
    '2330': 'å°ç©é›»',
    '2317': 'é´»æµ·',
    '2454': 'è¯ç™¼ç§‘',
    '2308': 'å°é”é›»',
    '2881': 'å¯Œé‚¦é‡‘',
    '2882': 'åœ‹æ³°é‡‘',
    '2412': 'ä¸­è¯é›»',
    '2303': 'è¯é›»',
    '1301': 'å°å¡‘',
    '1303': 'å—äº'
}

# ç¯„ä¾‹ï¼šæ‰¹æ¬¡æŸ¥è©¢
for code, name in LARGE_CAPS.items():
    print(f"æŸ¥è©¢ {name} ({code})")
    result = crawler.get_data(code)
    time.sleep(1)
```

---

## ğŸ ç¸½çµ

### æ ¸å¿ƒåŸå‰‡

1. **å„ªå…ˆä½¿ç”¨ AJAX**ï¼šæ¯” Selenium å¿« 20 å€
2. **å‹å–„çˆ¬èŸ²**ï¼šåŠ å»¶é²ã€ä¸éåº¦è«‹æ±‚
3. **éŒ¯èª¤è™•ç†**ï¼štry-exceptã€é‡è©¦æ©Ÿåˆ¶
4. **å„²å­˜é€²åº¦**ï¼šé¿å…é‡è·‘
5. **æ¸¬è©¦å…ˆè¡Œ**ï¼šå…ˆæ¸¬è©¦å–®ä¸€è³‡æ–™å†æ‰¹æ¬¡

### è¨˜ä½é€™å€‹æµç¨‹

```
æ‰¾ AJAX â†’ æˆªåœ–çµ¦ AI â†’ æ¸¬è©¦ç¨‹å¼ â†’ æ‰¹æ¬¡åŸ·è¡Œ â†’ å„²å­˜è³‡æ–™
```

### æœ€é‡è¦çš„ä¸€å¥è©±

> **ã€Œæ‰“é–‹ DevToolsï¼Œæ‰¾åˆ° AJAX è«‹æ±‚ï¼Œå°±æˆåŠŸäº† 90%ã€**

---

## ğŸ“ å•é¡Œæ’æŸ¥

é‡åˆ°å•é¡Œæ™‚çš„æª¢æŸ¥é †åºï¼š

```
1. status_code æ˜¯ 200 å—ï¼Ÿ
   â†“
2. response.text æœ‰å…§å®¹å—ï¼Ÿ
   â†“
3. åƒæ•¸æ‹¼å¯«æ­£ç¢ºå—ï¼Ÿ
   â†“
4. headers è¨­å®šäº†å—ï¼Ÿ
   â†“
5. HTML çµæ§‹æ”¹è®Šäº†å—ï¼Ÿ
```

---

**æœ€å¾Œæ›´æ–°**: 2024-11-04  
**ç‰ˆæœ¬**: 1.0  
**ä½œè€…ç­†è¨˜**: é€™ä»½æ–‡ä»¶è¨˜éŒ„äº†å¾é›¶é–‹å§‹å­¸ç¿’ MOPS çˆ¬èŸ²çš„å®Œæ•´éç¨‹ï¼Œæœªä¾†å¿˜è¨˜æ™‚å¯ä»¥éš¨æ™‚å›ä¾†æŸ¥é–±ã€‚
